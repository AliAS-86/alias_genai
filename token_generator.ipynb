{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alias-kube/dev/alias_genai/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'Ġam', 'Ġthe', 'Ġcaptain', 'Ġof', 'Ġmy']\n",
      "{'input_ids': tensor([[   40,   716,   262, 10654,   286,   616]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "Single token embedding: tensor([ 4.3597e-02, -1.3362e-01,  1.7140e-01,  1.1383e-02, -7.4586e-02,\n",
      "        -5.1723e-02, -2.9620e-01, -5.3332e-02,  4.0742e-02, -1.2568e-01,\n",
      "         1.3112e-01,  1.3011e-01,  2.3719e-02,  2.2207e-02,  1.0508e-01,\n",
      "         3.2130e-02,  7.2995e-02, -1.0549e-02,  5.5878e-02,  1.2675e-01,\n",
      "        -9.5042e-02, -1.6710e-01,  9.8741e-03,  5.8797e-02,  3.3012e-02,\n",
      "         1.1869e-02,  8.5770e-02,  7.8617e-02,  5.4794e-02, -1.5835e-01,\n",
      "        -2.0698e-02,  1.0220e-01, -1.8902e-02,  1.8424e-01, -7.7152e-02,\n",
      "        -2.9987e-02, -3.1549e-01,  1.7471e-02,  5.3715e-02, -2.1831e-02,\n",
      "        -4.0109e-02,  1.0515e-01,  2.6298e-02,  2.5054e-03,  1.6049e-01,\n",
      "        -1.1536e-01,  1.0039e-01, -2.1206e-01,  1.7320e-01, -1.6942e-01,\n",
      "         9.9444e-02, -1.7858e-01,  6.2710e-02, -2.1736e-02, -6.2229e-02,\n",
      "        -1.3480e-01, -4.8961e-02,  9.0328e-02,  1.2681e-01,  1.5385e-01,\n",
      "        -2.5024e-01, -9.4970e-02,  1.5753e-01,  1.5363e-01,  1.7460e-01,\n",
      "        -4.2338e-02,  3.3679e-02, -1.4376e-01,  1.2436e-01,  1.6831e-02,\n",
      "        -5.1425e-02, -6.4346e-02, -4.1208e-02,  2.0668e-02,  5.5843e-02,\n",
      "         3.1982e-01, -7.5061e-03,  1.4727e-01, -1.4003e-01, -1.7186e-02,\n",
      "         7.3568e-02,  7.0826e-02,  1.4572e-01,  4.7016e-02, -2.0701e-02,\n",
      "        -2.9839e-02, -7.4763e-02,  1.7496e-01,  1.0725e-01,  3.6353e-02,\n",
      "        -9.4771e-02, -2.2090e-01,  4.9920e-02,  4.3869e-02,  1.5504e-01,\n",
      "         6.5223e-03, -4.2525e-02, -5.8362e-02, -2.0536e-01,  2.1883e-01,\n",
      "         6.6058e-02, -1.0698e-01, -2.5640e-01, -1.8944e-01,  1.0691e-01,\n",
      "        -8.1136e-02, -7.1304e-02, -2.1613e-01,  9.4080e-02,  4.9951e-02,\n",
      "        -1.6087e-02, -2.9775e-02, -2.3024e-01,  1.1790e-01, -8.8062e-02,\n",
      "        -3.8150e-02, -2.1332e-02, -1.7964e-01, -1.7891e-01,  1.1408e-01,\n",
      "         7.1811e-02,  3.3935e-02,  8.5689e-02, -3.2873e-02, -1.7297e-02,\n",
      "        -4.5023e-02,  2.0233e-03, -9.4758e-02,  6.7992e-02,  1.0945e-01,\n",
      "         4.9945e-02,  4.3694e-02, -1.5039e-01, -1.2384e-02,  9.1948e-02,\n",
      "         1.1266e-01,  8.8075e-02,  5.9859e-02, -2.2421e-01, -5.2061e-02,\n",
      "         4.0957e-02, -6.0224e-02,  2.4268e-01, -1.0379e-02,  2.4300e-01,\n",
      "        -1.5219e-01,  8.4394e-02,  8.0745e-03,  1.7202e-02,  3.9055e-03,\n",
      "         9.9987e-02,  8.4059e-02, -6.8741e-04, -5.6599e-02,  2.6486e-02,\n",
      "         7.4595e-02, -1.1989e-01, -5.4350e-02, -9.0810e-03, -1.3946e-01,\n",
      "        -1.0114e-01, -2.1051e-01, -1.1696e-01,  6.2355e-03, -2.8889e-02,\n",
      "        -3.3925e-03,  2.9405e-02,  2.1939e-01, -1.7135e-02,  1.5316e-01,\n",
      "         2.9442e-02, -3.4294e-02,  2.0070e-01,  9.9615e-02,  1.3032e-01,\n",
      "        -5.4237e-02,  1.0586e-01,  2.2374e-01, -5.6125e-02,  1.2190e-02,\n",
      "         9.2134e-02, -1.5387e-01,  3.5364e-02, -4.5619e-02, -8.3459e-02,\n",
      "         1.4435e-01,  2.2979e-01,  1.8992e-02, -3.8708e-02,  4.2480e-02,\n",
      "         8.3255e-02, -2.3242e-02, -3.8370e-03,  1.2687e-01,  5.9963e-02,\n",
      "         1.5279e-01, -7.0918e-02, -7.2420e-02,  2.3759e-01,  2.9093e-01,\n",
      "        -6.6286e-02, -5.1193e-02,  1.3496e-01, -1.4821e-01,  1.2227e-01,\n",
      "        -1.5645e-01, -2.4454e-02, -1.7331e-02, -4.6217e-02,  1.0425e-01,\n",
      "         9.0096e-02,  9.2401e-02,  1.6734e-02, -1.4762e-01,  9.8698e-02,\n",
      "         6.6406e-02, -2.0324e-01,  1.7874e-01,  5.9526e-02, -5.0915e-02,\n",
      "        -6.5592e-02, -2.8174e-02,  3.9550e-02, -4.7164e-02, -1.7228e-01,\n",
      "        -3.8690e-02, -3.3560e-01, -6.4694e-03, -9.0391e-02,  1.8667e-01,\n",
      "         2.1250e-01, -3.3493e-02, -4.1711e-01,  8.1125e-02, -1.6400e-01,\n",
      "        -6.0117e-02, -2.0784e-01,  7.0788e-02,  4.7192e-02,  8.4239e-02,\n",
      "        -7.4982e-02,  4.5065e-02,  6.0574e-02, -1.0775e-01, -2.4500e-01,\n",
      "        -3.2072e-01, -1.8574e-02,  1.6794e-01, -3.7046e-02, -4.2375e-02,\n",
      "         1.8284e-01,  1.0553e-01, -3.3714e-02,  5.4862e-02, -1.4777e-01,\n",
      "        -3.1254e-01,  1.4620e-01,  6.2282e-02, -2.1800e-01, -1.3580e-01,\n",
      "        -1.7357e-01, -1.3954e-01, -5.8067e-02,  7.9407e-02,  5.1503e-02,\n",
      "         7.7283e-02,  1.6732e-01,  3.5356e-02, -1.8380e-02,  8.6979e-02,\n",
      "         9.2343e-02, -1.3266e-01,  2.9140e-02,  9.3220e-02,  7.7499e-02,\n",
      "        -6.0675e-02, -5.2616e-02,  3.8514e-02,  4.8106e-02,  1.5812e-01,\n",
      "        -1.0241e-01,  6.9754e-02, -1.2582e-01,  9.7206e-02, -9.0327e-02,\n",
      "         1.7780e-02, -6.4840e-02, -1.2312e-01, -2.7100e-02,  8.2326e-02,\n",
      "        -1.1843e-01,  2.9819e-02,  1.4644e-01,  9.1957e-02,  9.3017e-02,\n",
      "         8.9743e-02, -1.3474e-01,  8.8963e-02,  2.4167e-01, -1.2858e-01,\n",
      "         8.0374e-03, -1.1359e-01,  1.2256e-01, -6.9762e-02,  7.7948e-03,\n",
      "        -1.2186e-01,  1.4077e-02, -1.7316e-01,  1.6140e-01, -7.7474e-02,\n",
      "        -1.8529e-01, -5.0647e-03, -1.3799e-02, -4.9072e-02,  1.7725e-01,\n",
      "        -3.6505e-02,  1.4883e-01, -8.8570e-02, -9.8001e-02,  2.2143e-01,\n",
      "         1.2524e-01,  2.1771e-01, -2.2679e-01,  3.3521e-02, -3.5566e-02,\n",
      "         5.7052e-02, -2.5736e-01,  4.2172e-02,  8.9297e-02, -4.3560e-02,\n",
      "         9.5726e-02, -3.3618e-02,  3.1749e-02, -1.9078e-01, -1.1086e-01,\n",
      "         9.6581e-02,  6.2156e-02, -1.0751e-01,  7.9970e-02,  2.0242e-02,\n",
      "         1.1373e-01,  2.5237e-01, -9.8477e-02,  2.4788e-01, -1.6697e-01,\n",
      "        -2.2609e-02, -1.1015e-01, -1.5011e-01, -1.2949e-01, -1.4852e-01,\n",
      "        -1.4302e-01, -2.5128e-01,  1.0101e-01,  1.4685e-01, -5.1308e-02,\n",
      "        -7.3568e-02,  1.0433e-01,  2.7464e-02, -4.6600e-02, -1.8882e-01,\n",
      "         4.8298e-02,  7.2565e-02, -1.7732e-01,  2.8487e-01,  4.6357e-02,\n",
      "         1.1225e-01,  6.5260e-02,  6.0255e-02,  2.0889e-02,  5.9578e-02,\n",
      "        -8.8792e-02,  6.6665e-02,  3.6051e-02,  1.6070e-01,  1.7615e-01,\n",
      "         3.7781e-02,  1.0897e-01, -1.2443e-01,  1.3190e-01, -1.8640e-02,\n",
      "         1.1145e-01, -1.0390e-02, -2.0823e-01,  8.2255e-02, -4.4803e-02,\n",
      "         5.5846e-02,  4.2638e-02, -4.8383e-02, -2.4120e-01,  1.8870e-01,\n",
      "         1.8972e-01,  3.5114e-01,  9.7557e-02, -1.3338e-02,  7.4637e-02,\n",
      "         4.6519e-02, -5.8494e-02,  1.8051e-01,  2.5126e-01,  6.9878e-02,\n",
      "        -2.4131e-01,  6.7547e-02, -4.9450e-02, -1.3525e-01, -1.9206e-01,\n",
      "        -2.1893e-02, -2.1208e-01, -1.5082e-01, -1.4430e-01, -8.1611e-02,\n",
      "         1.5321e-01,  7.7734e-02,  7.2952e-02, -5.3687e-02, -8.3114e-02,\n",
      "         3.0469e-02, -7.6117e-02, -1.0702e-01,  1.1896e-01, -1.0799e-01,\n",
      "         2.0237e-01, -4.4439e-02, -3.3666e-02, -2.5860e-01, -1.0829e-01,\n",
      "        -2.8951e-02, -2.1717e-01,  4.0778e-02, -6.1743e-04, -7.9853e-02,\n",
      "        -2.5672e-01,  8.0365e-02, -9.9545e-02, -7.4206e-02,  1.4086e-01,\n",
      "        -9.4875e-02,  9.4284e-02,  2.2311e-01,  1.1953e-01, -2.2263e-01,\n",
      "        -3.1904e-01,  2.2350e-02, -1.8700e-01,  9.6193e-02, -2.2022e-01,\n",
      "         5.9796e-02,  1.4691e-01, -1.6955e-01, -3.0914e-02,  2.7172e-02,\n",
      "        -1.2075e-01,  1.4594e-01,  6.4270e-02, -2.5835e-02, -1.6725e-01,\n",
      "        -2.3180e-01, -5.9826e-03, -2.7001e-02, -9.8108e-02, -5.1368e-02,\n",
      "         4.5529e-02, -4.2868e-02,  1.3481e-01,  1.2618e-01,  6.0816e-02,\n",
      "         5.7715e-02, -8.6009e-02,  1.0279e-01, -1.1498e-01,  1.4037e-01,\n",
      "         1.1777e-01, -1.0046e-01, -4.1548e-04,  1.9928e-01, -5.2744e-02,\n",
      "        -1.8822e-01, -2.5487e-01, -3.9519e-02,  6.7558e-02,  1.7933e-01,\n",
      "         1.8995e-01, -1.5401e-01, -1.0706e-01,  1.1534e-01,  5.1660e-02,\n",
      "         1.7470e-01, -9.4869e-02,  4.9457e-02, -1.2744e-01,  1.4353e-01,\n",
      "         1.5959e-01, -3.9165e-02,  1.0172e-01, -1.7723e-04, -1.6045e-01,\n",
      "        -2.4458e-02, -2.0557e-01,  9.4858e-02,  2.8041e-02, -1.1606e-01,\n",
      "         5.6780e-02, -1.9888e-02,  1.4434e-01,  2.6990e-01, -9.7263e-02,\n",
      "         5.3700e-02,  8.9816e-02, -2.3780e-01, -1.8541e-01, -9.8025e-02,\n",
      "        -3.9065e-02,  1.3245e-01, -1.7471e-01,  2.0549e-01, -1.1043e-01,\n",
      "         1.0982e-01, -2.2187e-01, -4.8630e-02,  2.3435e-02,  3.2269e-02,\n",
      "        -6.4590e-02,  1.6681e-01,  8.9753e-02,  3.5908e-02, -4.9704e-02,\n",
      "         5.9427e-03,  2.2267e-01,  7.7900e-02, -9.1752e-02, -1.3319e-02,\n",
      "         4.2691e-02, -8.7649e-02,  1.4677e-01,  2.6365e-01,  1.5710e-01,\n",
      "         3.2118e-02, -8.4963e-02,  1.6606e-01,  1.1215e-01, -3.3374e-02,\n",
      "        -2.0942e-01,  2.1799e-01, -1.0557e-01,  2.3891e-01, -9.3367e-02,\n",
      "         9.1251e-02, -1.1026e-01, -1.6026e-01,  1.2414e-01,  2.0087e-01,\n",
      "        -3.9207e-02,  3.4648e-02,  6.5721e-02, -1.7977e-01,  1.7005e-01,\n",
      "        -3.1332e-02,  2.1850e-01, -1.6326e-01, -1.0484e-01,  5.7953e-02,\n",
      "        -6.9441e-02,  1.4787e-01,  5.6428e-02,  3.3587e-02,  1.5950e-03,\n",
      "        -9.2349e-02, -3.1981e-02, -6.6236e-02,  6.0565e-02,  4.1924e-02,\n",
      "         3.6732e-02, -8.3211e-02,  9.7096e-02,  1.8401e-01, -4.8545e-02,\n",
      "        -9.1544e-02, -5.1289e-02, -1.3897e-01, -1.0601e-01, -1.7748e-02,\n",
      "         1.0427e-01, -2.0290e-03, -1.4857e-01, -4.4273e-02, -1.0076e-01,\n",
      "        -1.2117e-01, -5.1259e-02,  4.2942e-02, -7.3444e-02,  1.1390e-02,\n",
      "         1.2501e-01, -8.7389e-02,  4.2975e-02,  1.2822e-01, -8.3261e-03,\n",
      "         1.6926e-01, -9.7499e-02,  1.7308e-01, -1.0153e-01,  1.7442e-01,\n",
      "        -4.4853e-02, -1.2279e-02,  1.3232e-01, -1.3659e-02, -2.6102e-03,\n",
      "        -8.7881e-02,  1.9154e-02,  8.5658e-03, -1.1190e-01, -1.9126e-01,\n",
      "         2.6407e-01, -1.7860e-02, -3.6391e-02,  2.7818e-01,  1.5246e-01,\n",
      "         1.5303e-01, -1.9027e-01,  1.3678e-01,  4.7595e-01, -7.8709e-02,\n",
      "         1.1673e-01,  2.7655e-01, -1.2352e-01, -4.8056e-02,  2.5530e-01,\n",
      "        -8.2550e-02, -1.2555e-02,  1.6555e-01,  8.7571e-02, -9.9925e-02,\n",
      "         1.3638e-01, -1.3530e-01, -7.9450e-02,  8.5901e-02, -7.3679e-02,\n",
      "         2.3879e-01, -1.0986e-01, -7.5477e-02,  8.2596e-02, -1.6213e-02,\n",
      "        -1.6882e-01,  1.8504e-02, -6.1672e-02,  9.7509e-03,  1.6554e-01,\n",
      "         1.6598e-01, -2.9735e-01, -8.0422e-02,  4.1195e-02, -4.8092e-03,\n",
      "         2.7965e-02,  1.0857e-01,  1.8492e-01, -2.0631e-01,  1.4734e-01,\n",
      "        -1.7691e-01,  3.7964e-02, -6.7050e-02, -1.0259e-01, -1.8744e-01,\n",
      "         1.4763e-02, -7.0955e-03,  1.6671e-01,  1.5108e-01, -8.8821e-02,\n",
      "        -2.1063e-01, -3.8922e-02,  4.1633e-02, -1.6415e-01, -1.1083e-01,\n",
      "         1.8760e-01, -1.6794e-01, -2.6718e-02, -6.0643e-02, -1.2748e-03,\n",
      "        -6.5193e-02, -6.2194e-03,  5.3421e-02, -5.9115e-02, -1.7556e-01,\n",
      "        -5.3469e-02,  5.3870e-02, -1.1444e-01, -5.2440e-03,  2.4211e-01,\n",
      "        -1.1528e-01,  1.5097e-01,  2.9986e-02, -8.2401e-02,  1.9279e-02,\n",
      "        -1.6484e-01,  9.8852e-02,  2.4693e-01, -5.4796e-02,  5.3308e-02,\n",
      "         2.0936e-01, -1.3597e-01, -2.6381e-01,  7.7265e-02, -8.3672e-02,\n",
      "        -8.6412e-02,  8.6243e-02,  7.0617e-02,  2.5108e-01, -1.1759e-01,\n",
      "        -1.0940e-02,  3.2585e-02, -1.3221e-01,  2.0782e-02,  1.3525e-01,\n",
      "        -6.7350e-02, -1.4120e-01, -2.4380e-02, -2.5295e-01, -1.5456e-01,\n",
      "        -3.6401e-02, -1.8973e-02,  6.2039e-02, -1.7402e-01, -6.1871e-02,\n",
      "        -1.5820e-01,  1.6288e-01,  1.4455e-01, -2.2736e-01, -9.8754e-02,\n",
      "        -1.0972e-02,  6.6798e-03,  1.8388e-01,  7.9490e-02, -9.8188e-02,\n",
      "         2.1645e-01,  1.8292e-01, -8.5166e-02,  2.1371e-02, -1.3153e-01,\n",
      "         1.0616e-03, -2.8394e-02, -8.4061e-02, -3.7916e-02,  5.0850e-02,\n",
      "        -9.4862e-02,  1.1955e-01, -8.5211e-02,  5.0058e-02,  1.8354e-01,\n",
      "         1.0265e-01,  1.1881e-01,  3.6359e-02,  3.5168e-02, -1.2312e-01,\n",
      "         5.4378e-02,  1.4028e-01,  6.2341e-02, -1.9025e-02,  5.1957e-02,\n",
      "         2.4170e-02, -1.8094e-01, -4.9482e-02,  1.8893e-01,  1.3104e-01,\n",
      "        -1.0263e-01, -2.8417e-01, -1.7344e-01, -1.2002e-02, -2.5962e-01,\n",
      "        -1.0419e-02,  1.2290e-02, -3.7975e-02], grad_fn=<SelectBackward0>)\n",
      "ste shape: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "# Generating one token at a time\n",
    "# Step 1: Load a tokenizer and a model\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load pretrained model and tokenizer using Huggingface\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Feed a a partial sentence to the tokenizer and tokenize it\n",
    "text = \"I am the captain of my\"\n",
    "# Tokenize the text input and map tokens to token IDs from the model vocabulary and return the tensors in PyTourch (PT) format\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Retrieve the token IDs from the tokenized data dictionary\n",
    "inputs[\"input_ids\"]\n",
    "\n",
    "# convert the input text to tokens (not token IDs but the tokens)\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens) # Output: ['I', 'Ġam', 'Ġthe', 'Ġcaptain', 'Ġof', 'Ġmy'], the Ġ indicate the presense of white space\n",
    "print(inputs)\n",
    "\n",
    "embeddings = model.transformer.wte # embedding layer\n",
    "dense_vector = embeddings(inputs[\"input_ids\"]) # passing the token IDs to the embedding layer to convert each token to it respective dense vector that is 768 values that capture the\n",
    "                                               # learned values about the token including context, syntax, semantic meaning, etc\n",
    "ste = dense_vector[0,3] # fetching only the dense vector for the 4th token, 'Ġcaptain' in this case, the tensor shape would be [768] meaning 1 raw = 1 token with 768 values\n",
    "# print(f\"dense vector: {dense_vector[0]}\")\n",
    "# print(dense_vector.shape)\n",
    "# print(dense_vector)\n",
    "\n",
    "print(f\"Single token embedding: {ste}\")\n",
    "print(f\"ste shape: {ste.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensor(40)</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tensor(716)</td>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tensor(262)</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tensor(10654)</td>\n",
       "      <td>captain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tensor(286)</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tensor(616)</td>\n",
       "      <td>my</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id     token\n",
       "0     tensor(40)         I\n",
       "1    tensor(716)        am\n",
       "2    tensor(262)       the\n",
       "3  tensor(10654)   captain\n",
       "4    tensor(286)        of\n",
       "5    tensor(616)        my"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the tokenizer by using pandas for visualization:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def view_tokenization(inputs):\n",
    "    return pd.DataFrame(\n",
    "        [(id, tokenizer.decode(id)) for id in inputs[\"input_ids\"][0]], columns=[\"id\", \"token\"],\n",
    "    )\n",
    "view_tokenization(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-102.6145, -101.7570, -105.0443,  ..., -104.0649, -107.5337,\n",
      "         -103.4872]])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>898</td>\n",
       "      <td>own</td>\n",
       "      <td>0.205202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>1074</td>\n",
       "      <td>team</td>\n",
       "      <td>0.135876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4074</th>\n",
       "      <td>4074</td>\n",
       "      <td>ship</td>\n",
       "      <td>0.066465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5462</th>\n",
       "      <td>5462</td>\n",
       "      <td>crew</td>\n",
       "      <td>0.038823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>1641</td>\n",
       "      <td>family</td>\n",
       "      <td>0.025252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>1499</td>\n",
       "      <td>country</td>\n",
       "      <td>0.019634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8244</th>\n",
       "      <td>8244</td>\n",
       "      <td>squad</td>\n",
       "      <td>0.017403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2151</th>\n",
       "      <td>2151</td>\n",
       "      <td>party</td>\n",
       "      <td>0.015524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>1664</td>\n",
       "      <td>company</td>\n",
       "      <td>0.012320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40733</th>\n",
       "      <td>40733</td>\n",
       "      <td>squadron</td>\n",
       "      <td>0.009413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id      token         p\n",
       "898      898        own  0.205202\n",
       "1074    1074       team  0.135876\n",
       "4074    4074       ship  0.066465\n",
       "5462    5462       crew  0.038823\n",
       "1641    1641     family  0.025252\n",
       "1499    1499    country  0.019634\n",
       "8244    8244      squad  0.017403\n",
       "2151    2151      party  0.015524\n",
       "1664    1664    company  0.012320\n",
       "40733  40733   squadron  0.009413"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3 Calculate the probability of the next token\n",
    "# Calculate the probabilities for the next token for all possible choices. We show the\n",
    "# top 5 choices and the corresponding words or subwords for these tokens.\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits[:, -1, :]\n",
    "    probabilities = torch.nn.functional.softmax(logits[0], dim=-1)\n",
    "    print(logits)\n",
    "def show_next_token_choices(probabilities, top_n=10):\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            (id, tokenizer.decode(id), p.item())\n",
    "            for id, p in enumerate(probabilities)\n",
    "            if p.item()\n",
    "        ],\n",
    "        columns=[\"id\", \"token\", \"p\"],\n",
    "    ).sort_values(\"p\", ascending=False)[:top_n]\n",
    "\n",
    "\n",
    "show_next_token_choices(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token id: 898\n",
      "Next token:  own\n"
     ]
    }
   ],
   "source": [
    "# Obtain the token id for the most probable next token\n",
    "next_token_id = torch.argmax(probabilities).item()\n",
    "\n",
    "print(f\"Next token id: {next_token_id}\")\n",
    "print(f\"Next token: {tokenizer.decode(next_token_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am the captain of my own own own'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We append the most likely token to the text.\n",
    "text = text + tokenizer.decode(898)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am the captain of my own own own\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Next token probabilities:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4074</th>\n",
       "      <td>4074</td>\n",
       "      <td>ship</td>\n",
       "      <td>0.122465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>1074</td>\n",
       "      <td>team</td>\n",
       "      <td>0.061103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>1664</td>\n",
       "      <td>company</td>\n",
       "      <td>0.030685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5462</th>\n",
       "      <td>5462</td>\n",
       "      <td>crew</td>\n",
       "      <td>0.026207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2151</th>\n",
       "      <td>2151</td>\n",
       "      <td>party</td>\n",
       "      <td>0.020760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>1641</td>\n",
       "      <td>family</td>\n",
       "      <td>0.018129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3430</th>\n",
       "      <td>3430</td>\n",
       "      <td>club</td>\n",
       "      <td>0.015509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>1499</td>\n",
       "      <td>country</td>\n",
       "      <td>0.012320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>,</td>\n",
       "      <td>0.012021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14893</th>\n",
       "      <td>14893</td>\n",
       "      <td>tribe</td>\n",
       "      <td>0.011046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id     token         p\n",
       "4074    4074      ship  0.122465\n",
       "1074    1074      team  0.061103\n",
       "1664    1664   company  0.030685\n",
       "5462    5462      crew  0.026207\n",
       "2151    2151     party  0.020760\n",
       "1641    1641    family  0.018129\n",
       "3430    3430      club  0.015509\n",
       "1499    1499   country  0.012320\n",
       "11        11         ,  0.012021\n",
       "14893  14893     tribe  0.011046"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'I am the captain of my own own own ship'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Press ctrl + enter to run this cell again and again to see how the text is generated.\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Show the text\n",
    "print(text)\n",
    "\n",
    "# Convert to tokens\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Calculate the probabilities for the next token and show the top 5 choices\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits[:, -1, :]\n",
    "    probabilities = torch.nn.functional.softmax(logits[0], dim=-1)\n",
    "\n",
    "display(Markdown(\"**Next token probabilities:**\"))\n",
    "display(show_next_token_choices(probabilities))\n",
    "\n",
    "# Choose the most likely token id and add it to the text\n",
    "next_token_id = torch.argmax(probabilities).item()\n",
    "text = text + tokenizer.decode(next_token_id)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I am a software enginner who uses AI to help me make better decisions. I am also a software developer who uses AI to help me make better decisions.\n",
       "\n",
       "I am a software enginner who uses AI to help me make better decisions. I am also a software developer who uses AI to help me make better decisions.\n",
       "\n",
       "I am a software enginner who uses AI to help me make better decisions. I am also a software developer who uses AI to help me make"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Start with some text and tokenize it\n",
    "text = \"I am a software enginner who uses AI to\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Use the `generate` method to generate lots of text\n",
    "output = model.generate(**inputs, max_length=100, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Show the generated text\n",
    "display(Markdown(tokenizer.decode(output[0])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
